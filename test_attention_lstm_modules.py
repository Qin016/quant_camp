#!/usr/bin/env python3
"""
Attention-LSTMç­–ç•¥æµ‹è¯•è„šæœ¬
éªŒè¯æ•°æ®åŠ è½½ã€ç‰¹å¾å·¥ç¨‹å’Œæ ‡ç­¾æ„é€ æ¨¡å—
"""

import pandas as pd
import numpy as np
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

from attention_lstm_strategy import (
    StrategyConfig, 
    DataLoader, 
    FeatureEngineering, 
    LabelConstructor,
    AttentionLSTMModel,
    AttentionLSTMStrategy
)

def create_sample_data(n_samples: int = 10000) -> pd.DataFrame:
    """åˆ›å»ºæ¨¡æ‹Ÿçš„L2è¡Œæƒ…æ•°æ®"""
    np.random.seed(42)
    
    # ç”ŸæˆåŸºç¡€ä»·æ ¼åºåˆ—
    base_price = 3000.0
    price_changes = np.random.normal(0, 0.001, n_samples)
    prices = base_price * np.exp(np.cumsum(price_changes))
    
    # åˆ›å»ºå®Œæ•´çš„tickæ•°æ®
    data = pd.DataFrame({
        'LASTPRICE': prices,
        'HIGHPRICE': prices * (1 + np.random.uniform(0, 0.002, n_samples)),
        'LOWPRICE': prices * (1 - np.random.uniform(0, 0.002, n_samples)),
        'TRADEVOLUME': np.random.poisson(100, n_samples),
        
        # ç›˜å£æ•°æ®
        'BUYPRICE01': prices * (1 - np.random.uniform(0.0001, 0.0005, n_samples)),
        'SELLPRICE01': prices * (1 + np.random.uniform(0.0001, 0.0005, n_samples)),
        'BUYVOLUME01': np.random.poisson(50, n_samples),
        'SELLVOLUME01': np.random.poisson(50, n_samples),
        
        # å¤šæ¡£ç›˜å£
        'BUYPRICE02': prices * (1 - np.random.uniform(0.0005, 0.001, n_samples)),
        'SELLPRICE02': prices * (1 + np.random.uniform(0.0005, 0.001, n_samples)),
        'BUYVOLUME02': np.random.poisson(30, n_samples),
        'SELLVOLUME02': np.random.poisson(30, n_samples),
        
        # æ—¶é—´æˆ³
        'timestamp': pd.date_range('2024-01-01 09:30:00', periods=n_samples, freq='1s')
    })
    
    return data

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½æ¨¡å—"""
    print("=== æµ‹è¯•æ•°æ®åŠ è½½æ¨¡å— ===")
    
    config = StrategyConfig()
    data_loader = DataLoader(config)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    sample_data = create_sample_data(1000)
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {sample_data.shape}")
    print(f"æ•°æ®åˆ—: {list(sample_data.columns)}")
    
    # æµ‹è¯•æ•°æ®é¢„å¤„ç†
    processed_data = data_loader._preprocess_data(sample_data)
    print(f"å¤„ç†åæ•°æ®å½¢çŠ¶: {processed_data.shape}")
    print(f"æ•°æ®ç±»å‹: {processed_data.dtypes}")
    
    # æ£€æŸ¥å¼‚å¸¸å€¼å¤„ç†
    print(f"ä»·æ ¼èŒƒå›´: {processed_data['LASTPRICE'].min():.2f} - {processed_data['LASTPRICE'].max():.2f}")
    print(f"æˆäº¤é‡èŒƒå›´: {processed_data['TRADEVOLUME'].min()} - {processed_data['TRADEVOLUME'].max()}")
    
    print("âœ“ æ•°æ®åŠ è½½æ¨¡å—æµ‹è¯•é€šè¿‡\n")

def test_feature_engineering():
    """æµ‹è¯•ç‰¹å¾å·¥ç¨‹æ¨¡å—"""
    print("=== æµ‹è¯•ç‰¹å¾å·¥ç¨‹æ¨¡å— ===")
    
    config = StrategyConfig()
    feature_engineer = FeatureEngineering(config)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    sample_data = create_sample_data(2000)
    
    # æµ‹è¯•å„ä¸ªç‰¹å¾æå–æ¨¡å—
    print("æµ‹è¯•ä»·æ ¼ç‰¹å¾æå–...")
    price_features = feature_engineer.extract_price_features(sample_data)
    print(f"ä»·æ ¼ç‰¹å¾æ•°é‡: {price_features.shape[1]}")
    print(f"ä»·æ ¼ç‰¹å¾åˆ—: {list(price_features.columns)}")
    
    print("æµ‹è¯•æˆäº¤é‡ç‰¹å¾æå–...")
    volume_features = feature_engineer.extract_volume_features(sample_data)
    print(f"æˆäº¤é‡ç‰¹å¾æ•°é‡: {volume_features.shape[1]}")
    print(f"æˆäº¤é‡ç‰¹å¾åˆ—: {list(volume_features.columns)}")
    
    print("æµ‹è¯•ç›˜å£ç‰¹å¾æå–...")
    orderbook_features = feature_engineer.extract_orderbook_features(sample_data)
    print(f"ç›˜å£ç‰¹å¾æ•°é‡: {orderbook_features.shape[1]}")
    print(f"ç›˜å£ç‰¹å¾åˆ—: {list(orderbook_features.columns)}")
    
    print("æµ‹è¯•æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾æå–...")
    technical_features = feature_engineer.extract_technical_indicators(sample_data)
    print(f"æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾æ•°é‡: {technical_features.shape[1]}")
    print(f"æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾åˆ—: {list(technical_features.columns)}")
    
    # æµ‹è¯•å®Œæ•´ç‰¹å¾å·¥ç¨‹
    print("æµ‹è¯•å®Œæ•´ç‰¹å¾å·¥ç¨‹æµç¨‹...")
    all_features = feature_engineer.engineer_features(sample_data)
    print(f"æ€»ç‰¹å¾æ•°é‡: {all_features.shape[1]}")
    print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {all_features.shape}")
    
    # æµ‹è¯•æ»‘åŠ¨çª—å£å½’ä¸€åŒ–
    print("æµ‹è¯•æ»‘åŠ¨çª—å£å½’ä¸€åŒ–...")
    normalized_features = feature_engineer.rolling_normalize_features(all_features)
    print(f"å½’ä¸€åŒ–åå½¢çŠ¶: {normalized_features.shape}")
    print(f"å½’ä¸€åŒ–åç»Ÿè®¡: å‡å€¼={normalized_features.mean():.4f}, æ ‡å‡†å·®={normalized_features.std():.4f}")
    
    # æ£€æŸ¥ç‰¹å¾è´¨é‡
    print("ç‰¹å¾è´¨é‡æ£€æŸ¥:")
    print(f"  æ— ç©·å¤§å€¼æ•°é‡: {np.sum(np.isinf(normalized_features))}")
    print(f"  NaNå€¼æ•°é‡: {np.sum(np.isnan(normalized_features))}")
    
    print("âœ“ ç‰¹å¾å·¥ç¨‹æ¨¡å—æµ‹è¯•é€šè¿‡\n")

def test_label_construction():
    """æµ‹è¯•æ ‡ç­¾æ„é€ æ¨¡å—"""
    print("=== æµ‹è¯•æ ‡ç­¾æ„é€ æ¨¡å— ===")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    sample_data = create_sample_data(1000)
    
    # æµ‹è¯•åŸºç¡€æ ‡ç­¾æ„é€ 
    label_constructor = LabelConstructor(k_ticks=5, threshold=0.0001)
    labels = label_constructor.construct_labels(sample_data)
    
    print(f"æ ‡ç­¾æ•°ç»„é•¿åº¦: {len(labels)}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ:")
    unique_labels, counts = np.unique(labels, return_counts=True)
    for label, count in zip(unique_labels, counts):
        label_name = ['ä¸‹è·Œ', 'éœ‡è¡', 'ä¸Šæ¶¨'][label]
        print(f"  {label_name}({label}): {count} ({count/len(labels)*100:.1f}%)")
    
    # æµ‹è¯•å¸¦ç½®ä¿¡åº¦çš„æ ‡ç­¾æ„é€ 
    labels_conf, confidence = label_constructor.construct_labels_with_confidence(sample_data)
    print(f"ç½®ä¿¡åº¦ç»Ÿè®¡: å‡å€¼={confidence.mean():.3f}, æ ‡å‡†å·®={confidence.std():.3f}")
    print(f"ç½®ä¿¡åº¦èŒƒå›´: {confidence.min():.3f} - {confidence.max():.3f}")
    
    # éªŒè¯æ— æœªæ¥æ•°æ®æ³„éœ²
    is_valid = label_constructor.validate_no_future_leakage(sample_data, labels)
    print(f"æœªæ¥æ•°æ®æ³„éœ²æ£€æŸ¥: {'é€šè¿‡' if is_valid else 'å¤±è´¥'}")
    
    # æ£€æŸ¥æ ‡ç­¾çš„æ—¶é—´ä¸€è‡´æ€§
    print("æ ‡ç­¾æ—¶é—´ä¸€è‡´æ€§æ£€æŸ¥:")
    # æœ€åkä¸ªtickåº”è¯¥éƒ½æ˜¯éœ‡è¡æ ‡ç­¾
    last_k_labels = labels[-label_constructor.k_ticks:]
    all_neutral = np.all(last_k_labels == 1)
    print(f"  æœ€å{label_constructor.k_ticks}ä¸ªæ ‡ç­¾æ˜¯å¦ä¸ºéœ‡è¡: {'æ˜¯' if all_neutral else 'å¦'}")
    
    print("âœ“ æ ‡ç­¾æ„é€ æ¨¡å—æµ‹è¯•é€šè¿‡\n")

def test_model_architecture():
    """æµ‹è¯•æ¨¡å‹æ¶æ„"""
    print("=== æµ‹è¯•æ¨¡å‹æ¶æ„ ===")
    
    config = StrategyConfig()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    sample_data = create_sample_data(500)
    feature_engineer = FeatureEngineering(config)
    features = feature_engineer.engineer_features(sample_data)
    
    print(f"è¾“å…¥ç‰¹å¾ç»´åº¦: {features.shape[1]}")
    
    # åˆ›å»ºæ¨¡å‹
    model = AttentionLSTMModel(config, input_size=features.shape[1])
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    import torch
    device = torch.device('cpu')
    model = model.to(device)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    test_input = torch.FloatTensor(features[:config.sequence_length]).unsqueeze(0)
    print(f"æµ‹è¯•è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output, attention_weights = model(test_input)
    
    print(f"æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")
    print(f"è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ: {output.numpy()[0]}")
    print(f"æ³¨æ„åŠ›æƒé‡ç»Ÿè®¡: æœ€å°={attention_weights.min():.4f}, æœ€å¤§={attention_weights.max():.4f}")
    
    print("âœ“ æ¨¡å‹æ¶æ„æµ‹è¯•é€šè¿‡\n")

def test_end_to_end_pipeline():
    """æµ‹è¯•ç«¯åˆ°ç«¯æµç¨‹"""
    print("=== æµ‹è¯•ç«¯åˆ°ç«¯æµç¨‹ ===")
    
    try:
        # åˆ›å»ºç­–ç•¥é…ç½®
        config = StrategyConfig()
        config.epochs = 2  # å‡å°‘è®­ç»ƒè½®æ¬¡ç”¨äºæµ‹è¯•
        config.batch_size = 32
        
        # åˆ›å»ºç­–ç•¥å®ä¾‹
        strategy = AttentionLSTMStrategy(config)
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®
        train_data = {
            'IF': create_sample_data(5000),
            'IC': create_sample_data(5000)
        }
        
        print(f"è®­ç»ƒæ•°æ®: {list(train_data.keys())}")
        print(f"æ¯ä¸ªåˆçº¦æ•°æ®é‡: {[len(df) for df in train_data.values()]}")
        
        # æµ‹è¯•æ•°æ®é¢„å¤„ç†
        for contract, df in train_data.items():
            features, labels = strategy._prepare_contract_data(df, contract)
            print(f"{contract} é¢„å¤„ç†ç»“æœ:")
            print(f"  ç‰¹å¾å½¢çŠ¶: {features.shape}")
            print(f"  æ ‡ç­¾æ•°é‡: {len(labels)}")
            print(f"  æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(labels)}")
        
        print("âœ“ ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•é€šè¿‡\n")
        
    except Exception as e:
        print(f"âŒ ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹Attention-LSTMç­–ç•¥æ¨¡å—æµ‹è¯•")
    print("="*60)
    
    test_data_loading()
    test_feature_engineering()
    test_label_construction()
    test_model_architecture()
    test_end_to_end_pipeline()
    
    print("="*60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    
    print("\nğŸ“Š æ¨¡å—è®¾è®¡æ€»ç»“:")
    print("1. æ•°æ®åŠ è½½: âœ“ æ”¯æŒå¼‚å¸¸å€¼å¤„ç†ã€ç¼ºå¤±å€¼å¡«å……ã€æ•°æ®ç±»å‹ä¼˜åŒ–")
    print("2. ç‰¹å¾å·¥ç¨‹: âœ“ å®ç°20+ç‰¹å¾ï¼ŒåŒ…æ‹¬ä»·æ ¼ã€æˆäº¤é‡ã€ç›˜å£ã€æŠ€æœ¯æŒ‡æ ‡")
    print("3. æ»‘åŠ¨çª—å£å½’ä¸€åŒ–: âœ“ é¿å…look-ahead biasçš„å½’ä¸€åŒ–æ–¹æ³•")
    print("4. æ ‡ç­¾æ„é€ : âœ“ åŸºäºæœªæ¥kä¸ªtickä¸­é—´ä»·ï¼Œä¸¥æ ¼é¿å…æ•°æ®æ³„éœ²")
    print("5. æ¨¡å‹æ¶æ„: âœ“ åŠ¨æ€ç‰¹å¾æ•°é‡æ”¯æŒï¼Œæ³¨æ„åŠ›æœºåˆ¶å¢å¼º")

if __name__ == "__main__":
    main()
